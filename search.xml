<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker部署GitLab并实现基本配置]]></title>
    <url>%2Ftechnique%2FDocker%E9%83%A8%E7%BD%B2GitLab%E5%B9%B6%E5%AE%9E%E7%8E%B0%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[本地GitLab的安装需要部署各种依赖和其他服务，费时且麻烦，而直接使用Docker进行容器化部署则省时简单，只要运行一行命令即可使用。本文记录了在实验室内网环境下利用Docker搭建源码托管工具GitLab，并列出一些必要的个性化配置项。 准备工作操作系统由于Ubuntu系统在Docker环境下兼容性更高，选择了 Ubuntu 18.04 LTS 作为操作系统环境。 安装DockerDocker环境的安装十分简单，在这里不详述，根据Docker —— 从入门到实践选择对应操作系统的安装教程即可。 安装GitLab-ce 拉取GitLab-ce镜像，查看镜像信息 $ docker pull gitlab/gitlab-ce$ docker image ls 创建并启动一个GitLab容器，:后内容不要修改 $ GITLAB_HOME = /home/docker/gitlab # 建立gitlab本地目录$ docker run -d \--hostname gitlab.example.com\ # 指定容器域名,创建镜像仓库用-p 8443:443 \ # 容器443端口映射到主机8443端口(https)-p 8080:80 \ # 容器80端口映射到主机8080端口(http)-p 2222:22 \ # 容器22端口映射到主机2222端口(ssh)--name gitlab \ # 容器名称--restart always \ # 容器退出后自动重启-v $GITLAB_HOME/config:/etc/gitlab \ # 挂载本地目录到容器配置目录-v $GITLAB_HOME/logs:/var/log/gitlab \ # 挂载本地目录到容器日志目录-v $GITLAB_HOME/data:/var/opt/gitlab \ # 挂载本地目录到容器数据目录gitlab/gitlab-ce:latest # 使用的镜像:版本 查看容器运行情况 $ docker container ls 配置GitLab]]></content>
      <categories>
        <category>technique</category>
      </categories>
      <tags>
        <tag>Tutorial</tag>
        <tag>Linux</tag>
        <tag>Docker</tag>
        <tag>GitLab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯泊松分解变分推断笔记]]></title>
    <url>%2Fresearch%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%8A%E6%9D%BE%E5%88%86%E8%A7%A3%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[贝叶斯泊松分解一般形式因为可以对观测数据进行灵活的符合实际的建模（不同的概率分布假设），贝叶斯概率分解模型已经成为了最常见的矩阵/张量分解方法。其中，贝叶斯泊松分解模型一方面可以对计数值（count data）进行有效的建模，另一方面得益于其非负的分解结构，可以用于替代传统的非负矩阵分解模型（NMF），因而被广泛应用于推荐系统、因子分析和聚类分析中。常见的贝叶斯泊松矩阵分解模型如下，其中观测值$x_{ij}$服从泊松分布，而其分解得到的因子矩阵的值则服从共轭的Gamma分布： \begin{equation} \begin{split} &x_{ij}=\sum_{k=1}^{K}z_{ijk}, z_{ijk}\sim\text{Pois}(u_{ik}v_{jk}), \\ &u_{ik}\sim\text{Gamma}(a^{(u)},\frac{b^{(u)}}{a^{(u)}}),\\ &v_{jk}\sim\text{Gamma}(a^{(v)},\frac{b^{(v)}}{a^{(v)}}).\\ \end{split} \end{equation} 其中Gamma分布的概率密度函数如下所示，$\alpha\in\mathbb{R}_{+}$为shape参数，$\beta\in\mathbb{R}_{+}$为scale参数，$\Gamma(n+1)=n!$为gamma函数： \begin{equation} \text{Gamma}(x;\alpha,\beta)=\text{exp}\left((\alpha-1)\text{ln}x-\frac{x}{\beta}-\text{ln}\Gamma(\alpha)-\alpha\text{ln}\beta\right) \end{equation}Binary形式变分推断变分更新公式上述模型的联合概率分布函数为 \begin{equation} p(X,Z,U,V)=p(X\mid Z)p(Z\mid U,V)p(U)p(V) \end{equation}其对数形式展开如下 \begin{equation} \begin{split} \text{ln}p(X,Z,U,V)=&\sum_{i}\sum_{j}\sum_{k}\left(-u_{ik}v_{jk}+z_{ijk}\text{ln}(u_{ik}v_{jk})-\text{ln}\Gamma(z_{ijk}+1)\right) \\ &+\sum_{i}\sum_{j}\left((a^{(u)}-1)\text{ln}u_{ik}-\frac{a^{(u)}}{b^{(u)}}u_{ik}-\text{ln}\Gamma(a^{(u)})-a^{(u)}\text{ln}\frac{b^{(u)}}{a^{(u)}}\right) \\ &+\sum_{i}\sum_{j}\left((a^{(v)}-1)\text{ln}v_{jk}-\frac{a^{(v)}}{b^{(v)}}v_{jk}-\text{ln}\Gamma(a^{(v)})-a^{(v)}\text{ln}\frac{b^{(v)}}{a^{(v)}}\right) \\ \end{split} \end{equation}与此同时，对后验概率分布的变分近似分布进行分解，得到 \begin{equation} \begin{split} q(Z,U,V)&=q(Z)q(U)q(V) \\ &=\prod_{i,j}q_{\boldsymbol{z}_{ij}}(\boldsymbol{z}_{ij})\prod_{i,k}q_{u_{ik}}(u_{ik})\prod_{j,k}q_{v_{jk}}(v_{jk}) \end{split} \end{equation}根据变分贝叶斯推断笔记中的公式(3)，我们可以对各个因子的最优化形式进行推导。首先，对于因子$q_{\boldsymbol{z}_{ij}}(\boldsymbol{z}_{ij})$，有 \begin{equation} \begin{split} \text{ln}q_{\boldsymbol{z}_{ij}}^{*}(\boldsymbol{z}_{ij})&=\mathbb{E}_{(\Theta\backslash \boldsymbol{z}_{ij})}[\text{ln}p(X,Z,U,V)]+\text{const} \\ &=\mathbb{E}_{(\Theta\backslash \boldsymbol{z}_{ij})}\left[\sum_{k}\left(-\text{ln}\Gamma(z_{ijk}+1)+z_{ijk}\left(\text{ln}u_{ik}+\text{ln}v_{jk}\right)\right)\right]+\text{const} \\ &=\sum_{k}\left(-\text{ln}\Gamma(z_{ijk}+1)+z_{ijk}\left(\mathbb{E}[\text{ln}u_{ik}]+\mathbb{E}[\text{ln}v_{jk}]\right)\right)+\text{const} \\ &=\sum_{k}\left(-\text{ln}\Gamma(z_{ijk}+1)+z_{ijk}\text{ln}e^{\mathbb{E}[\text{ln}u_{ik}]+\mathbb{E}[\text{ln}v_{jk}]}\right)+\text{const} \\ \end{split} \end{equation}辅助变量$\boldsymbol{z}_{ij}$的后验为多项式分布，其参数为 \begin{equation} \phi_{ijk}^{*}=\frac{e^{\mathbb{E}[\text{ln}u_{ik}]+\mathbb{E}[\text{ln}v_{jk}]}}{\sum_{k}e^{\mathbb{E}[\text{ln}u_{ik}]+\mathbb{E}[\text{ln}v_{jk}]}} \end{equation}因此$z_{ijk}$的更新公式为 \begin{equation} \mathbb{E}[z_{ijk}]=x_{ij}\phi_{ijk}^{*} \end{equation}进一步地，对于因子$q_{u_{ik}}(u_{ik})$，有 \begin{equation} \begin{split} \text{ln}q_{u_{ik}}^{*}(u_{ik})&=\mathbb{E}_{(\Theta\backslash u_{ik})}[\text{ln}p(X,Z,U,V)]+\text{const} \\ &=\mathbb{E}_{(\Theta\backslash u_{ik})}\left[\left(a^{(u)}+\sum_{j}z_{ijk}-1\right)\text{ln}u_{ik}-\left(\frac{a^{(u)}}{b^{(u)}}+\sum_{k}v_{jk}\right)u_{ik}\right]+\text{const} \\ &=\left(a^{(u)}+\sum_{j}\mathbb{E}[z_{ijk}]-1\right)\text{ln}u_{ik}-\left(\frac{a^{(u)}}{b^{(u)}}+\sum_{k}\mathbb{E}[v_{jk}]\right)u_{ik}+\text{const} \\ \end{split} \end{equation}由共轭性，$q_{u_{ik}}(u_{ik})$仍然是Gamma分布，其参数为 \begin{equation} \begin{split} \alpha_{ik}^{(u)*}&=a^{(u)}+\sum_{j}\mathbb{E}[z_{ijk}],\\ \beta_{ik}^{(u)*}&=\left(\frac{a^{(u)}}{b^{(u)}}+\sum_{k}\mathbb{E}[v_{jk}]\right)^{-1},\\ \end{split} \end{equation}因此$u_{ik}$的更新公式为 \begin{equation} \begin{split} \mathbb{E}[u_{ik}]&=\alpha_{ik}^{(u)*}\beta_{ik}^{(u)*} \\ \mathbb{E}[\text{ln}u_{ik}]&=\psi(\alpha_{ik}^{(u)*})+\text{ln}\beta_{ik}^{(u)*} \end{split} \end{equation}最后，因子$q_{v_{jk}}(v_{jk})$的计算与因子$q_{u_{ik}}(u_{ik})$类似。 变分下界计算变分下界的计算公式如下： \begin{equation} \begin{split} \mathcal{L}(q)&=\mathbb{E}_{q}[\text{ln}p(X,\Theta)]+H(q(\Theta)) \end{split} \end{equation}其中$H(q(\Theta))=-\mathbb{E}_{q}[\text{ln}q(\Theta)]$，因此我们可以计算变分下界，其中$\sum_{i}\sum_{j}\sum_{k}\mathbb{E}\left[\text{ln}\Gamma(z_{ijk}+1)\right]$项可以在计算过程中消去 \begin{equation} \begin{split} \mathcal{L}(q)=&-\sum_{i}\sum_{j}\sum_{k}\mathbb{E}[u_{ik}]\mathbb{E}[v_{jk}] \\ &+\sum_{i}\sum_{k}\mathbb{E}[\text{ln}u_{ik}]\left(a^{(u)}-1+\sum_{j}\mathbb{E}[z_{ijk}]\right) \\ &+\sum_{j}\sum_{k}\mathbb{E}[\text{ln}v_{jk}]\left(a^{(v)}-1+\sum_{i}\mathbb{E}[z_{ijk}]\right) \\ &+\sum_{i}\sum_{k}\left(-\frac{a^{(u)}}{b^{(u)}}\mathbb{E}[u_{ik}]-\text{ln}\Gamma(a^{(u)})-a^{(u)}\text{ln}\frac{b^{(u)}}{a^{(u)}}\right) \\ &+\sum_{j}\sum_{k}\left(-\frac{a^{(v)}}{b^{(v)}}\mathbb{E}[v_{jk}]-\text{ln}\Gamma(a^{(v)})-a^{(v)}\text{ln}\frac{b^{(v)}}{a^{(v)}}\right) \\ &+\sum_{i}\sum_{j}\left(-\text{ln}\Gamma(x_{ij}+1)-\sum_{k}\mathbb{E}[z_{ijk}]\text{ln}\phi_{ijk}^{*}\right) \\ &+\sum_{i}\sum_{k}\left(-(\alpha_{ik}^{(u)*}-1)\psi(\alpha_{ik}^{(u)*})+\text{ln}\beta_{ik}^{(u)*}+\alpha_{ik}^{(u)*}+\text{ln}\Gamma(\alpha_{ik}^{(u)*})\right) \\ &+\sum_{j}\sum_{k}\left(-(\alpha_{jk}^{(v)*}-1)\psi(\alpha_{jk}^{(v)*})+\text{ln}\beta_{jk}^{(v)*}+\alpha_{jk}^{(v)*}+\text{ln}\Gamma(\alpha_{jk}^{(v)*})\right) \\ \end{split} \end{equation}参考 Prem Gopalan, Jake M. Hofman, David M. Blei. “Scalable recommendation with hierarchical poisson factorization”. In UAI, 2015.]]></content>
      <categories>
        <category>research</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Bayesian</tag>
        <tag>Variational Inference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变分贝叶斯推断笔记]]></title>
    <url>%2Fresearch%2F%E5%8F%98%E5%88%86%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[变分贝叶斯推断问题描述假设当前有一个贝叶斯模型，且其中的参数都有相应的先验分布。同时，模型中还可能有潜变量，将其与各种参数标记为$\Theta$。同样地，把所有观测变量集合标记为$\mathcal{Y}$。因此，我们希望找到分布$q(\Theta)$来逼近真实后验分布$p(\Theta\mid\mathcal{Y})$，而这可以通过最小化KL散度实现，也即： \begin{equation} \begin{split} \text{KL}(q(\Theta)\|p(\Theta|\mathcal{Y}))&=\int q(\Theta)\text{ln}\left\{\frac{q(\Theta)}{p(\Theta\mid\mathcal{Y})}\right\}d\Theta \\ &=\text{ln}p(\mathcal{Y})-\int q(\Theta)\text{ln}\left\{\frac{p(\mathcal{Y},\Theta)}{q(\Theta)}\right\}d\Theta \end{split} \end{equation}其中$\text{ln}p(\mathcal{Y})$表示模型证据（Evidence），则其下界（lower bound）可以定义为$\mathcal{L}(q)=\int q(\Theta)\text{ln}\{\frac{p(\mathcal{Y},\Theta)}{q(\Theta)}\}d\Theta$。因为模型证据是一个常量，当KL散度为0时，下界出现最大值，这就意味着$q(\Theta)=p(\mathcal{Y},\Theta)$。 平均场理论根据平均场理论（mean field theory），我们假设变分分布$q(\Theta)$可以被分解成各组变量分布的乘积，即可以写作： \begin{equation} q(\Theta)=\prod_{j}^{M}q_j(\Theta_j) \end{equation}这是针对该分布的唯一假设，其中每一个独立因子$q_j(\Theta_j)$的特定函数形式可以具体地一个个推导出来。通过最大化下届$\mathcal{L}(q)$，第$j$个因子的优化形式由下式给出： \begin{equation} \text{ln}q_j(\Theta_j)=\mathbb{E}_{q(\Theta\backslash\Theta_j)}\left[\text{ln}p(\mathcal{Y},\Theta)\right]+\text{const} \end{equation}其中$\mathbb{E}_{q(\Theta\backslash\Theta_j)}\left[\cdot\right]$表示关于除$\Theta_j$外所有变量的$q$分布的期望。因为所有参数的分布都属于指数族分布，且都与其父节点共轭，因此我们能够通过公式(3)推导出$\Theta$中每个参数的后验分布更新的近似形式。 变分下界在模型计算时，可以通过直接计算变分下界$\mathcal{L}(q)$来判断算法是否收敛，因为在每一次迭代中变分下界不减。变分下界可以通过下式进行计算 \begin{equation} \begin{split} \mathcal{L}(q)&=\int q(\Theta)\text{ln}\left\{\frac{p(\mathcal{Y},\Theta)}{p(\Theta)}\right\}d\Theta \\ &=\mathbb{E}_{q}[\text{ln}p(\mathcal{Y},\Theta)]-\mathbb{E}_{q}[\text{ln}q(\Theta)] \\ &=\mathbb{E}_{q}[\text{ln}p(\mathcal{Y}\mid\Theta)]+\mathbb{E}_{q}[\text{ln}p(\Theta)]-\sum_{j}\mathbb{E}_{q_j(\Theta_j)}[\text{ln}q_j(\Theta_j)] \end{split} \end{equation}其中第一项表示联合分布的后验期望，而第二项则表示后验$q$分布的熵$H(q(\Theta))=-\mathbb{E}_{q}[\text{ln}p(q(\Theta))]$。 例子-一元高斯模型模型推断给定一组观测数据值$\mathcal{D}=\{x_1,x_2,\dots,x_N\}$，假定数据是独立地从高斯分布中抽取的，目标是通过最大化后验分布推断得到均值参数$\mu$和精度参数$\tau$，其似然函数为 \begin{equation} \begin{split} p(\mathcal{D}\mid \mu,\tau)&=\prod_{i=1}^{N}\left(\frac{\tau}{2\pi}\right)^{\frac{1}{2}}\text{exp}\left\{-\frac{\tau(x_i-\mu)^2}{2}\right\} \\ &=\left(\frac{\tau}{2\pi}\right)^{\frac{N}{2}}\text{exp}\left\{-\frac{\tau}{2}\sum_{i=1}^{N}(x_i-\mu)^2\right\} \end{split} \end{equation}同时引入参数$\mu$和$\tau$的共轭先验分布，其形式为 \begin{equation} \begin{split} p(\mu\mid\tau)&= \mathcal{N}(\mu\mid\mu_{0},\left[\lambda_{0}\tau\right]^{-1})\\ p(\tau)&= \text{Gam}(\tau\mid a_0,b_0) \end{split} \end{equation}则其联合分布可以表示为 \begin{equation} \begin{split} p(\mathcal{D},\mu,\tau)=p(\mathcal{D}\mid\mu,\tau)p(\mu\mid\tau)p(\tau) \end{split} \end{equation}对后验概率分布的变分近似进行分解 \begin{equation} q(\mu,\tau)=q_{\mu}(\mu)q_{\tau}(\tau) \end{equation}根据公式(3)，可以推导出各个因子的优化形式，对于$q_{\mu}(\mu)$，我们有 \begin{equation} \begin{split} \text{ln}q_{\mu}^{*}(\mu)&=\mathbb{E}_{\tau}[\text{ln}p(\mathcal{D}\mid\mu,\tau)+\text{ln}p(\mu\mid\tau)]+\text{const} \\ &=-\frac{\mathbb{E}[\tau]}{2}\left\{\sum_{i=1}^{N}(x_i-\mu)^2+\lambda_0(\mu-\mu_0)^2\right\}+\text{const} \\ &=-\frac{1}{2}\left\{(\lambda_0+N)\mathbb{E}[\tau]\mu^2-2(\lambda_0\mu_0+N\bar{x})\mathbb{E}[\tau]\mu\right\}+\text{const} \end{split} \end{equation}由共轭性，可以看到$q_{\mu}(\mu)$是一个高斯分布，其参数为 \begin{equation} \begin{split} \lambda^{*}&=(\lambda_0+N)\mathbb{E}[\tau] \\ \mu^{*}&=[\lambda^*]^{-1}(\lambda_0\mu_0+N\bar{x})\mathbb{E}[\tau] =\frac{\lambda_0\mu_0+N\bar{x}}{\lambda_0+N} \end{split} \end{equation}同理，因子$q_{\tau}(\tau)$的最优形式为 \begin{equation} \begin{split} \text{ln}q_{\tau}^{*}(\tau)&=\mathbb{E}_{\mu}[\text{ln}p(\mathcal{D}\mid\mu,\tau)+\text{ln}p(\mu\mid\tau)+\text{ln}p(\tau)]+\text{const} \\ &=(a_0-1+\frac{N+1}{2})\text{ln}\tau-\left\{b_0+\frac{1}{2}\mathbb{E}\mu\left[\sum_{i=1}^{N}(x_i-\mu)^2+\lambda_0(\mu-\mu_0)^2\right]\right\}\tau+\text{const} \end{split} \end{equation}因此$q_{\tau}(\tau)$是一个Gamma分布，参数为 \begin{equation} \begin{split} a^{*}&=a_0+\frac{N+1}{2} \\ b^{*}&=b_0+\frac{1}{2}\mathbb{E}\mu\left[\sum_{i=1}^{N}(x_i-\mu)^2+\lambda_0(\mu-\mu_0)^2\right] \end{split} \end{equation}为了评估模型的收敛性，我们进一步计算变分下界，其公式如下 \begin{equation} \begin{split} \mathcal{L}(q)&=\mathbb{E}_{q}[\text{ln}p(\mathcal{Y},\Theta)]-\mathbb{E}_{q}[\text{ln}q(\Theta)] \\ &=\mathbb{E}_{q(\mu,\tau)}[\text{ln}p(\mathcal{D}\mid\mu,\tau)]+\mathbb{E}_{q(\mu,\tau)}[\text{ln}p(\mu\mid\tau)]+\mathbb{E}_{q(\tau)}[\text{ln}p(\tau)]-\mathbb{E}_{q(\mu)}[\text{ln}q(\mu)]-\mathbb{E}_{q(\tau)}[\text{ln}q(\tau)] \end{split} \end{equation}上式各个部分可以通过下面公式计算得到，其中$\mathbb{E}_q[\mu^2]=\mathbb{E}_{\mu}[\mu^2]=(\mathbb{E}[\mu])^2+\text{Var}[\mu]$，$\mathbb{E}_q[\text{ln}\tau]=\mathbb{E}_{\tau}[\text{ln}\tau]=\psi(a)-\text{ln}b$，$\psi(\cdot)$为双伽马函数（digamma function）而$\Gamma(\cdot)$为伽马函数。 \begin{equation} \begin{split} \mathbb{E}_{q}[\text{ln}p(\mathcal{D}\mid\mu,\tau)]&=-\frac{N}{2}\text{ln}(2\pi)+\frac{N}{2}\mathbb{E}_q[\text{ln}\tau]-\frac{1}{2}\mathbb{E}_q[\tau]\mathbb{E}_q[\sum_{i=1}^{N}(x_i-\mu)^2] \\ &=-\frac{N}{2}\text{ln}(2\pi)+\frac{N}{2}(\psi(a^*)-\text{ln}(b^*))-\frac{a^*}{2b^*}\left\{\sum_{i=1}^{N}x_i^2-2\mu^*\sum_{i=1}^{N}x_i+\mathbb{E}_q[\mu^2]\right\} \\ \end{split} \end{equation} \begin{equation} \begin{split} \mathbb{E}_{q}[\text{ln}p(\mu\mid\tau)]&=-\frac{1}{2}\text{ln}(2\pi)+\frac{1}{2}\mathbb{E}_q[\text{ln}(\lambda_0\tau)]-\frac{\lambda_0}{2}\mathbb{E}_q[\tau]\mathbb{E}_q[(\mu-\mu_0)^2] \\ &=-\frac{1}{2}\text{ln}(2\pi)+\frac{\text{ln}\lambda_0}{2}+\frac{1}{2}(\psi(a^*)-\text{ln}(b^*))-\frac{\lambda_0 a^*}{2b^*}\left\{\mathbb{E}_q[\mu^2]-2\mu_0\mu^*+\mu_0^2\right\} \\ \end{split} \end{equation} \begin{equation} \begin{split} \mathbb{E}_{q}[\text{ln}p(\tau)]&=-\text{ln}\Gamma(a_0)+a_0\text{ln}b_0+(a_0-1)\mathbb{E}_q[\text{ln}\tau]-b_0\mathbb{E}_q[\tau] \\ &=-\text{ln}\Gamma(a_0)+a_0\text{ln}b_0+(a_0-1)(\psi(a^*)-\text{ln}b^*)-b_0\frac{a^*}{b^*} \end{split} \end{equation} \begin{equation} \begin{split} -\mathbb{E}_{q}[\text{ln}q(\mu)]&=\frac{1}{2}\text{ln}(2\pi)-\frac{\text{ln}\lambda^*}{2}+\frac{\lambda^*}{2}\left\{\mathbb{E}_q[\mu^2]-2(\mu^*)^2+(\mu^*)^2\right\} \\ &=\frac{1}{2}\text{ln}(2\pi)-\frac{\text{ln}\lambda^*}{2}+\frac{1}{2} \end{split} \end{equation} \begin{equation} \begin{split} -\mathbb{E}_{q}[\text{ln}q(\tau)]&=\text{ln}\Gamma(a^*)-(a^*-1)\psi(a^*)-\text{ln}b^*+a^* \end{split} \end{equation}至此，我们得到了关于参数$\mu$与$\tau$最优分布的表达式，且各自依赖于另一个分布的计算所得的一阶矩或二阶矩。因此通过初始化参数的值，可以通过不断迭代计算出后验分布。 在此例子中，由于模型简单且参数数量只有两个，因此我们可以通过直接求解上式的因子找到显示解。首先我们可以通过设置无信息先验（noninformative prior）来简化上述表达式，也即$\mu_0=\lambda_0=a_0=b_0=0$。根据Gamma分布的均值计算公式，我们有 \begin{equation} \begin{split} \frac{1}{\mathbb{E}[\tau]}=\frac{b^*}{a^*}=\mathbb{E}\left[\frac{1}{N+1}\sum_{i=1}^{N}(x_i-\mu)^2\right]=\frac{N}{N+1}(\overline{x^{2}}-2\bar{x}\mathbb{E}[\mu]+\mathbb{E}[\mu^2]) \end{split} \end{equation}由公式(10)，我们可以获得近似分布$q_\mu(\mu)$的一阶矩与二阶矩 \begin{equation} \begin{split} \mathbb{E}[\mu]=\bar{x}, \mathbb{E}[\mu^2]=\bar{x}^2+\frac{1}{N\mathbb{E}[\tau]} \end{split} \end{equation}将其代入公式(19)，可以解出$\mathbb{E}[\tau]$ \begin{equation} \frac{1}{\mathbb{E}[\tau]}=\overline{x^2}-\bar{x}^2 \end{equation}=\frac{1}{N}\sum_{i=1}^{N}(x_i-\bar{x})^2代码示例上节一元高斯的求解MATLAB代码如下所示，首先通过随机数生成器生成$N$个高斯分布的随机数作为模型观测值clear;% 生成高斯分布随机数mu_real = 1;tau_real = 1.5;N = 200;D = normrnd(mu_real, sqrt(1./tau_real), [N,1]);sumD = sum(D);sumD2 = sum(D.^2);mu_est = mean(D); % 公式(20)tau_est = 1./(mean(D.^2)-mean(D).^2); % 公式(21)% 查看该一元高斯的分布图像x_real=-4+mu_real:0.1:mu_real+4;y_real=normpdf(x_real,mu_real,sqrt(1./tau_real));x_est=-4+mu_est:0.1:mu_est+4;y_est=normpdf(x_est,mu_est,sqrt(1./tau_est));figure;plot(x_real,y_real,'-r.',x_est,y_est,'--b.');grid; 初始化模型参数和超参数% 初始化参数和超参数mu0 = 1e-6; % 无信息先验lambda0 = 1e-6; % 无信息先验a0 = 1e-6; % 无信息先验b0 = 1e-6; % 无信息先验mu = randn();mus(1) = mu;tau = rand();taus(1) = tau;LB = 0; % 变分下界tol = 1e-5; % 收敛允许误差maxiters = 100; % 迭代最大次数 可视化求解过程，将分别绘出模型参数的真值与估计值比较图、变分下界变化图和精度参数$\tau$的后验分布变化图% 可视化求解过程scrnsz = get(0,'ScreenSize');h = figure('Position',[scrnsz(3)*0.25 scrnsz(4)*0.25 scrnsz(3)*0.5 scrnsz(4)*0.5]);set(0,'CurrentFigure',h);subplot(2,2,1); plot(mu_real, '-r.','LineWidth',1.5,'MarkerSize',10 ); title('Model parameter \mu'); xlabel('Iteration'); grid on;subplot(2,2,2); plot(tau_real, '-r.','LineWidth',1.5,'MarkerSize',10 ); title('Model parameter \tau'); xlabel('Iteration'); grid on;subplot(2,2,3); plot(LB, '-r.','LineWidth',1.5,'MarkerSize',10 ); title('Lower bound'); xlabel('Iteration'); grid on;subplot(2,2,4); plot(0:0.1:20, gampdf(0:0.1:20, a0, 1./b0), 'r-'); title('Posterior pdf'); xlabel('Noise precision \tau'); grid on;set(findall(h,'type','text'),'fontSize',12);drawnow; 模型迭代求解% 模型求解for it=1:maxiters % 更新参数 mu，公式(10) lambda_new = (lambda0+N)*tau; mu_new = (1./lambda_new)*(lambda0*mu0+sumD)*tau; mu = mu_new; mus(it+1) = mu; E_mu2 = mu_new^2+1./lambda_new; % E[mu^2]=E[mu]^2+Var[mu] % 更新参数 tau，公式(12) a_new = a0+(N+1)./2; b_new = b0+0.5*(sumD2-2*(sumD+lambda0*mu0)*mu+(N+lambda0)*E_mu2+lambda0*(mu0^2)); tau = a_new./b_new; taus(it+1) = tau; E_lntau = psi(a_new)-safelog(b_new); % E[ln(tau)]=psi(a)-ln(b) % 评估变分下界，公式(13) E_pD = -0.5*N*safelog(2*pi)+0.5*N*E_lntau-0.5*tau*(sumD2-2*mu*sumD+E_mu2); E_pmu = -0.5*safelog(2*pi)+0.5*E_lntau+0.5*safelog(lambda0)-0.5*lambda0*tau*(E_mu2-2*mu0*mu+mu0^2); E_ptau = -safelog(gamma(a0))+a0*safelog(b0)+(a0-1)*E_lntau-b0*tau; E_qmu = 0.5*safelog(2*pi)-0.5*safelog(lambda_new)+0.5; E_qtau = safelog(gamma(a_new))-(a_new-1)*psi(a_new)-safelog(b_new)+a_new; LB(it) = E_pD + E_pmu + E_ptau + E_qmu + E_qtau; % 可视化求解过程 set(0,'CurrentFigure',h); subplot(2,2,1); plot(mu_real*ones(1,it+1), '-r.','LineWidth',1.5,'MarkerSize',10);hold on;plot(mu_est*ones(1,it+1), '--r.','LineWidth',1.5,'MarkerSize',10 );hold on;plot(mus, '-b.','LineWidth',1.5,'MarkerSize',10);hold off; title('Model parameter \mu'); xlabel('Iteration'); grid on; subplot(2,2,2); plot(tau_real*ones(1,it+1), '-r.','LineWidth',1.5,'MarkerSize',10);hold on;plot(tau_est*ones(1,it+1), '--r.','LineWidth',1.5,'MarkerSize',10 );hold on;plot(taus, '-b.','LineWidth',1.5,'MarkerSize',10);hold off; title('Model parameter \tau'); xlabel('Iteration'); grid on; subplot(2,2,3); plot(LB, '-r.','LineWidth',1.5,'MarkerSize',10); title('Lower bound'); xlabel('Iteration'); grid on; subplot(2,2,4); plot(0:0.05:2*tau, gampdf(0:0.05:2*tau, a_new, 1./b_new), '-r.', 'LineWidth',1.5); title('Posterior pdf'); xlabel('Noise precision \tau'); grid on; set(findall(h,'type','text'),'fontSize',12); drawnow; % 判断模型是否收敛 if it&gt;3 LB_change = -1*(LB(it) - LB(it-1))/LB(3); else LB_change = NaN; end if it&gt;10 &amp;&amp; (abs(LB_change) &lt; tol) disp('Converged!'); break; endendfunction y = safelog(x)x(x&lt;1e-300)=1e-200;x(x&gt;1e300)=1e300;y=log(x);end 求解结果如下图所示，左上与右上两图中的红色实线表示生成观测数据模型参数的真值，红色虚线表示利用观测数据显式计算得到的模型参数的估计值，蓝色实线表示利用变分推断迭代求解的参数估计值。在本例子中，由于推断公式较简单，$\mathbb{E}[\mu]$的计算不依赖于$\mathbb{E}[\tau]$，所以算法在第二次迭代就已经收敛到显式计算的估计值。 例子-混合高斯模型模型推断代码示例参考 Christopher M. Bishop. “Pattern recognition and machine learning.” Springer, 2006. PRML Errata 1st: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/prml-errata-1st-20110921.pdf Qibin Zhao, Liqing Zhang, and Andrzej Cichocki. “Bayesian CP factorization of incomplete tensors with automatic rank determination.” IEEE transactions on pattern analysis and machine intelligence, 2015. https://github.com/qbzhao/BCPF https://en.wikipedia.org/wiki/Gamma_distribution https://en.wikipedia.org/wiki/Conjugate_prior]]></content>
      <categories>
        <category>research</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Bayesian</tag>
        <tag>Variational Inference</tag>
      </tags>
  </entry>
</search>
