<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Spark笔记-玩转RDD操作"><meta name="keywords" content="Big-Data,Spark,Python,RDD"><meta name="author" content="yxnchen,undefined"><meta name="copyright" content="yxnchen"><title>Spark笔记-玩转RDD操作 | YXN's Blog</title><link rel="shortcut icon" href="/favicon-32x32.png"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#RDD简介"><span class="toc-number">1.</span> <span class="toc-text">RDD简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RDD的创建"><span class="toc-number">2.</span> <span class="toc-text">RDD的创建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#通过数据集合转化为RDD"><span class="toc-number">2.1.</span> <span class="toc-text">通过数据集合转化为RDD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#从HDFS数据源或本地文件创建"><span class="toc-number">2.2.</span> <span class="toc-text">从HDFS数据源或本地文件创建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#从其他数据库读取数据创建"><span class="toc-number">2.3.</span> <span class="toc-text">从其他数据库读取数据创建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用数据流创建"><span class="toc-number">2.4.</span> <span class="toc-text">使用数据流创建</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#一般RDD的转换操作（Transformation）"><span class="toc-number">3.</span> <span class="toc-text">一般RDD的转换操作（Transformation）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#flatMap"><span class="toc-number">3.1.</span> <span class="toc-text">flatMap()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#map"><span class="toc-number">3.2.</span> <span class="toc-text">map()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#filter"><span class="toc-number">3.3.</span> <span class="toc-text">filter()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#distinct"><span class="toc-number">3.4.</span> <span class="toc-text">distinct()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sample"><span class="toc-number">3.5.</span> <span class="toc-text">sample()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#union"><span class="toc-number">3.6.</span> <span class="toc-text">union()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#intersection"><span class="toc-number">3.7.</span> <span class="toc-text">intersection()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#subtract"><span class="toc-number">3.8.</span> <span class="toc-text">subtract()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cartesian"><span class="toc-number">3.9.</span> <span class="toc-text">cartesian()</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#键值对RDD的转换操作（Transformation）"><span class="toc-number">4.</span> <span class="toc-text">键值对RDD的转换操作（Transformation）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#map-1"><span class="toc-number">4.1.</span> <span class="toc-text">map()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reduceByKey"><span class="toc-number">4.2.</span> <span class="toc-text">reduceByKey()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#groupByKey"><span class="toc-number">4.3.</span> <span class="toc-text">groupByKey()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#aggregateByKey"><span class="toc-number">4.4.</span> <span class="toc-text">aggregateByKey()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#combineByKey"><span class="toc-number">4.5.</span> <span class="toc-text">combineByKey()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sortByKey"><span class="toc-number">4.6.</span> <span class="toc-text">sortByKey()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#keys-与values"><span class="toc-number">4.7.</span> <span class="toc-text">keys()与values()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mapValues"><span class="toc-number">4.8.</span> <span class="toc-text">mapValues()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#join"><span class="toc-number">4.9.</span> <span class="toc-text">join()</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RDD的行动操作（Action）"><span class="toc-number">5.</span> <span class="toc-text">RDD的行动操作（Action）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#count"><span class="toc-number">5.1.</span> <span class="toc-text">count()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#collect"><span class="toc-number">5.2.</span> <span class="toc-text">collect()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#first"><span class="toc-number">5.3.</span> <span class="toc-text">first()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#top"><span class="toc-number">5.4.</span> <span class="toc-text">top()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#take"><span class="toc-number">5.5.</span> <span class="toc-text">take()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#takeOrdered"><span class="toc-number">5.6.</span> <span class="toc-text">takeOrdered()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#takeSample"><span class="toc-number">5.7.</span> <span class="toc-text">takeSample()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lookup"><span class="toc-number">5.8.</span> <span class="toc-text">lookup()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#foreach"><span class="toc-number">5.9.</span> <span class="toc-text">foreach()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reduce"><span class="toc-number">5.10.</span> <span class="toc-text">reduce()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#aggregate"><span class="toc-number">5.11.</span> <span class="toc-text">aggregate()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#countByKey"><span class="toc-number">5.12.</span> <span class="toc-text">countByKey()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#countByValue"><span class="toc-number">5.13.</span> <span class="toc-text">countByValue()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#saveAsTextFile"><span class="toc-number">5.14.</span> <span class="toc-text">saveAsTextFile()</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RDD的持久化"><span class="toc-number">6.</span> <span class="toc-text">RDD的持久化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#persist"><span class="toc-number">6.1.</span> <span class="toc-text">persist()</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">yxnchen</div><div class="author-info__description text-center">We choose to go to the moon.</div><div class="follow-button"><a href="https://github.com/yxnchen" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">7</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">15</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">2</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Bookmarks</div><a class="author-info-links__name text-center" href="https://molunerfinn.com" target="_blank">Molunerfinn</a><a class="author-info-links__name text-center" href="https://spencerwoo.com" target="_blank">Spencer Woo</a><a class="author-info-links__name text-center" href="https://wyydsb.xin" target="_blank">Gunjianpan</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://gh-io-1257470807.cos.ap-guangzhou.myqcloud.com/BFR1_Moon.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">YXN's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Posts</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">👨‍🚀</a></span></div><div id="post-info"><div id="post-title">Spark笔记-玩转RDD操作</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-04-23</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/technique/">technique</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">3.4k</span><span class="post-meta__separator">|</span><span>阅读时长: 12 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><blockquote>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>RDD（Resilient Distributed Dataset）译作<strong>弹性分布式数据集</strong>，是Spark中最常用的数据抽象，是一个只可读、可分区、可并行计算的数据集合。RDD允许将工作集缓存在内存中进行复用，大大地提升了查询速度。</p>
</blockquote>
<a id="more"></a>
<h1 id="RDD简介"><a href="#RDD简介" class="headerlink" title="RDD简介"></a>RDD简介</h1><ul>
<li>MapReduce 在面对日益复杂的业务逻辑时已经表现出严重的不足：1）维护成本高昂，每一次数据处理都需要编写复杂的Map和Reduce步骤，中间某一步骤出错就要重试以处理异常；2）难以上手，处理性能睇；</li>
<li>因此人们想出用<strong>有向无环图</strong>（DAG）来抽象表达复杂的数据处理逻辑，各个数据处理步骤表示成图中的节点与边依赖关系，形成数据流的抽象表示，而把复杂的性能优化提交给后台自动处理；</li>
<li>RDD也即分布式对象集合，是一个<strong>只读</strong>的分区记录集合，每个RDD可以划分成多个<strong>分区</strong>，每个分区就是数据集的一部分，同时不同分区可以存储在集群中不同的节点上，从而利用集群节点优势进行并行计算；</li>
<li>RDD提供了丰富的操作以支持常见的数据处理，即“转换”（Transformation）和“行动”（Action）两种类型操作<ul>
<li><strong>转换</strong>操作指定RDD的依赖关系，通过接受RDD并返回RDD；</li>
<li><strong>行动</strong>操作执行计算并指定输出的形式，通过接受RDD返回输出值或结果（非RDD）；</li>
</ul>
</li>
<li>通过Spark的API可以使用不同的语言调用RDD的操作，常见过程流程如下：<ul>
<li>从各种数据源创建RDD；</li>
<li>对RDD指定一系列的转换操作；</li>
<li>最后调用行动操作，输出结果或写入外部数据源；</li>
</ul>
</li>
<li>RDD操作的<strong>惰性机制</strong>，是指在RDD执行操作时，只有触发行动操作才会做真正的计算，而在行动前的所有转换操作都只是记录下相互的依赖关系，形成数据流的管道化（pipeline），而不会做真正的计算；</li>
</ul>
<h1 id="RDD的创建"><a href="#RDD的创建" class="headerlink" title="RDD的创建"></a>RDD的创建</h1><h2 id="通过数据集合转化为RDD"><a href="#通过数据集合转化为RDD" class="headerlink" title="通过数据集合转化为RDD"></a>通过数据集合转化为RDD</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc = SparkContext(<span class="string">"local"</span>, <span class="string">"create_rdd"</span>)</span><br><span class="line">ints = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>]</span><br><span class="line">rdd = sc.parallelize(ints)</span><br><span class="line">strings = [<span class="string">'spark'</span>,<span class="string">'hadoop'</span>,<span class="string">'rdd'</span>]</span><br><span class="line">rdd = sc.parallelize(strings)</span><br></pre></td></tr></table></figure>
<h2 id="从HDFS数据源或本地文件创建"><a href="#从HDFS数据源或本地文件创建" class="headerlink" title="从HDFS数据源或本地文件创建"></a>从HDFS数据源或本地文件创建</h2><p>从一篇CNN新闻报道 <a href="https://us.cnn.com/2019/04/11/tech/uber-lyft-businesses/index.html" target="_blank" rel="noopener">Uber and Lyft may look the same, but their visions are not</a> 中抽取新闻主体并放到文件<code>news_sep.txt</code>中，一段话为一行。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">At first blush, it can be hard to tell Uber and ...</span><br><span class="line">But look under the hood and there&apos;s a clear difference ...</span><br><span class="line">Uber filed paperwork on Thursday for what is expected ...</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>通过指定文件路径读取为RDD：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc = SparkContext(<span class="string">"local"</span>, <span class="string">"create_rdd"</span>)</span><br><span class="line"><span class="comment"># 从本地文件系统地址加载</span></span><br><span class="line">rdd = sc.textFile(<span class="string">"file:///path/to/news_sep.txt"</span>)</span><br><span class="line"><span class="comment"># 从分布式文件系统HDFS地址加载，下面三种方式等价</span></span><br><span class="line">rdd = sc.textFile(<span class="string">"hdfs://localhost:9000/path/to/news_sep.txt"</span>)</span><br><span class="line">rdd = sc.textFile(<span class="string">"/path/to/news_sep.txt"</span>)</span><br><span class="line">rdd = sc.textFile(<span class="string">"news_sep.txt"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="从其他数据库读取数据创建"><a href="#从其他数据库读取数据创建" class="headerlink" title="从其他数据库读取数据创建"></a>从其他数据库读取数据创建</h2><p>（待更新）</p>
<h2 id="使用数据流创建"><a href="#使用数据流创建" class="headerlink" title="使用数据流创建"></a>使用数据流创建</h2><p>结合流数据处理技术，如Spark Streaming、Kafka以及flume等，通过接收实时的输入数据流创建RDD。</p>
<h1 id="一般RDD的转换操作（Transformation）"><a href="#一般RDD的转换操作（Transformation）" class="headerlink" title="一般RDD的转换操作（Transformation）"></a>一般RDD的转换操作（Transformation）</h1><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations" target="_blank" rel="noopener">官方API文档</a>详细列出转换操作函数，下面简单介绍RDD常用的转换操作：</p>
<h2 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap()"></a>flatMap()</h2><p><code>flatMap(func)</code>：对于每一个输入元素，通过指定函数映射到0或多个元素，输出新的RDD。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将上述新闻文本的每一行每一段话根据空格进行分词</span></span><br><span class="line">rdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>))</span><br></pre></td></tr></table></figure></p>
<h2 id="map"><a href="#map" class="headerlink" title="map()"></a>map()</h2><p><code>map(func)</code>：对于每一个输入元素，通过执行指定函数映射到唯一输出（1v1关系），产生新的RDD<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对上述分词结果的RDD中每一个单词去除标点符号，并转化为小写</span></span><br><span class="line">rdd = rdd.map(<span class="keyword">lambda</span> w: w.strip(<span class="string">"\""</span>).strip(<span class="string">","</span>).strip(<span class="string">"."</span>).lower())</span><br></pre></td></tr></table></figure></p>
<h2 id="filter"><a href="#filter" class="headerlink" title="filter()"></a>filter()</h2><p><code>filter(func)</code>：是对RDD元素进行过滤，把经过指定函数后返回值为true的元素组成一个新的RDD。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对上述结果筛选掉一些常见的介词，如</span></span><br><span class="line">rm_words = [<span class="string">"and"</span>,<span class="string">"in"</span>,<span class="string">"at"</span>,<span class="string">"a"</span>,<span class="string">"an"</span>,<span class="string">"is"</span>,<span class="string">"are"</span>,<span class="string">"may"</span>,<span class="string">"that"</span>,<span class="string">"this"</span>,<span class="string">"to"</span>,<span class="string">"as"</span>,<span class="string">"with"</span>,<span class="string">"of"</span>,<span class="string">"can"</span>,<span class="string">"be"</span>]</span><br><span class="line">rdd = rdd.filter(<span class="keyword">lambda</span> w: w <span class="keyword">not</span> <span class="keyword">in</span> rm_words)</span><br></pre></td></tr></table></figure></p>
<h2 id="distinct"><a href="#distinct" class="headerlink" title="distinct()"></a>distinct()</h2><p><code>distinct([numPartitions])</code>：对数据进行去重，返回一个新的RDD，numPartitions参数用于设置任务并行数。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输出新闻中所有提及过的单词</span></span><br><span class="line">rdd = rdd.distinct(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="sample"><a href="#sample" class="headerlink" title="sample()"></a>sample()</h2><p><code>sample(withReplacement,fraction,seed=None)</code>：对数据进行采样，withReplacement参数表示是否放回抽样；fraction参数表示抽样比例；seed表示随机种子。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机抽取新闻提及部分单词</span></span><br><span class="line">rdd1 = rdd.sample(<span class="keyword">False</span>, <span class="number">0.2</span>, <span class="number">2019</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="union"><a href="#union" class="headerlink" title="union()"></a>union()</h2><p><code>union(otherRDD)</code>：可以与另一个RDD数据集合并，返回一个新的RDD。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([<span class="string">"union"</span>,<span class="string">"other"</span>,<span class="string">"rdd"</span>])</span><br><span class="line">rdd = rdd.union(rdd1)</span><br></pre></td></tr></table></figure></p>
<h2 id="intersection"><a href="#intersection" class="headerlink" title="intersection()"></a>intersection()</h2><p><code>intersection(otherRDD)</code>：可以与另一个RDD数据集进行求交集计算，返回新的RDD。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">rdd2 = sc.parallelize([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>])</span><br><span class="line">rdd3 = rdd1.intersection(rdd2)</span><br><span class="line"><span class="comment"># 最后rdd3的结果为 [3,4,5]</span></span><br></pre></td></tr></table></figure></p>
<h2 id="subtract"><a href="#subtract" class="headerlink" title="subtract()"></a>subtract()</h2><p><code>subtract(otherRDD,[numPartitions])</code>：是对otherRDD进行减法操作，将原始RDD的元素减去新输入RDD的元素，将差值返回新RDD。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">rdd2 = sc.parallelize([<span class="number">2</span>,<span class="number">4</span>])</span><br><span class="line">rdd3 = rdd1.subtract(rdd2)</span><br><span class="line"><span class="comment"># 最后rdd3的结果为 [1,3,5]</span></span><br></pre></td></tr></table></figure></p>
<h2 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian()"></a>cartesian()</h2><p><code>cartesian(otherRDD)</code>：可以对两个RDD数据集U，V求笛卡尔积，返回一个新的RDD数据集，其中每个元素为(u,v)。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">rdd2 = sc.parallelize([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">rdd3 = rdd1.cartesian(rdd2)</span><br><span class="line"><span class="comment"># 最后rdd3的结果为 [(1,3),(1,4),(2,3),(2,4)]</span></span><br></pre></td></tr></table></figure></p>
<h1 id="键值对RDD的转换操作（Transformation）"><a href="#键值对RDD的转换操作（Transformation）" class="headerlink" title="键值对RDD的转换操作（Transformation）"></a>键值对RDD的转换操作（Transformation）</h1><h2 id="map-1"><a href="#map-1" class="headerlink" title="map()"></a>map()</h2><p><code>map(func)</code>：操作可以将一般RDD转换为键值对RDD，元素变成(K,V)。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对上述新闻分词结果的RDD中每一个单词转化为(w,1)键值对</span></span><br><span class="line">rdd = rdd.map(<span class="keyword">lambda</span> w: (w,<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<h2 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey()"></a>reduceByKey()</h2><p><code>reduceByKey(func,[numPartitions])</code>：可以对具有相同键的值进行合并，返回一个新的键值对RDD，numPartitions用于设置任务并行数。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对新闻出现过的单词进行词频统计</span></span><br><span class="line">rdd = rdd.reduceByKey(<span class="keyword">lambda</span> w1,w2: w1+w2)</span><br></pre></td></tr></table></figure></p>
<h2 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey()"></a>groupByKey()</h2><p><code>groupByKey([numPartitions])</code>：可以对具有相同键的值进行分组，返回一个元素为(K,[Iterable])的键值对RDD，numPartitions用于指定任务并行数，默认为8。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对新闻出现过的单词进行词频统计</span></span><br><span class="line">rdd = rdd.groupByKey()</span><br></pre></td></tr></table></figure></p>
<h2 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey()"></a>aggregateByKey()</h2><p><code>aggregateByKey(zeroValue,seqFunc,combFunc,[numPartitions])</code>：可以对具有相同键的值进行聚合，把(K,V)键值对RDD转换为新的(K,U)键值对RDD，其中U由给定的combFunc和中立零值zeroValue聚合而成，U可以有与V不一致的形式；</p>
<ul>
<li>zeroValue 可以是0如果聚合的目的是求和，可以是List如果目的是对值进行统合，可以是Set如果目的是聚合唯一值；</li>
<li>seqFunc: (U,V) =&gt; U 对分区内的元素进行聚合（操作发生在每个分区内部）；</li>
<li>combFunc: (U,U) =&gt; U 对不同分区的聚合结果做进一步的聚合（操作发生在全部分区的聚合结果间）；</li>
<li>numPartitions 用于设置任务并行数；</li>
</ul>
<blockquote>
<p>为什么使用两个函数？见<a href="https://backtobazics.com/big-data/spark/apache-spark-aggregatebykey-example/" target="_blank" rel="noopener">Apache Spark aggregateByKey Example</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 同样对新闻出现过的单词进行词频统计</span></span><br><span class="line">rdd = rdd.aggregateByKey(<span class="number">0</span>,<span class="keyword">lambda</span> v1,v2: v1+v2, <span class="keyword">lambda</span> a1,a2: a1+a2)</span><br></pre></td></tr></table></figure>
<h2 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey()"></a>combineByKey()</h2><p><code>combineByKey(createCombiner,mergeValue,mergeCombiners,[numPartitions])</code>：可以对具有相同键的值按照自定义函数的逻辑进行聚合，把(K,V)键值对RDD转换为新的(K,U)键值对RDD，U可以有与V不一致的形式；</p>
<ul>
<li>createCombiner: V =&gt; C 创建新的聚合器方便后续步骤操作，对原始值进行附加操作并返回，跟flatMap()类似；</li>
<li>mergeValue: (C,V) =&gt; C 对分区内的元素进行聚合（操作发生在每个分区内部）；</li>
<li>mergeCombiners: (C,C) =&gt; C 对不同分区的聚合结果做进一步的聚合（操作发生在全部分区的聚合结果间）；</li>
<li>numPartitions 用于设置任务并行数；</li>
</ul>
<blockquote>
<p>groupByKey(), groupByKey(), aggregateByKey() 等都不同程度上依赖于 combineByKey() 操作</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对词频统计的结果求频数均值，例如</span></span><br><span class="line">strings = [<span class="string">'spark'</span>,<span class="string">'hadoop'</span>,<span class="string">'rdd'</span>,<span class="string">'rdd'</span>,<span class="string">'spark'</span>,<span class="string">'rdd'</span>]</span><br><span class="line">rdd = sc.parallelize(strings)</span><br><span class="line">rdd = rdd.map(<span class="keyword">lambda</span> w: (w,<span class="number">1</span>))</span><br><span class="line">rdd = rdd.reduceByKey(<span class="keyword">lambda</span> v1,v2: v1 + v2)</span><br><span class="line">rdd = rdd.combineByKey(<span class="keyword">lambda</span> v: (<span class="number">1</span>,v), <span class="keyword">lambda</span> c, v: (c[<span class="number">0</span>]+<span class="number">1</span>,c[<span class="number">1</span>]+v), <span class="keyword">lambda</span> c1, c2: (c1[<span class="number">0</span>]+c1[<span class="number">0</span>],c2[<span class="number">1</span>]+c2[<span class="number">1</span>]))</span><br><span class="line"><span class="comment"># 最后的结果为 [('spark', (1, 2)), ('hadoop', (1, 1)), ('rdd', (1, 3))]</span></span><br></pre></td></tr></table></figure>
<h2 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey()"></a>sortByKey()</h2><p><code>sortByKey(ascending,[numPartitions])</code>：可以对键值对RDD按照键进行排序操作，其中K需要实现Ordered方法。</p>
<ul>
<li>ascending 决定RDD中的元素按升序还是降序排序，默认是True升序；</li>
<li>numPartitions 用于设置任务并行数；</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对词频统计结果按降序排序（先把K-V值互换）</span></span><br><span class="line">rdd = rdd.map(<span class="keyword">lambda</span> v: (v[<span class="number">1</span>],v[<span class="number">0</span>]))</span><br><span class="line">rdd = rdd.sortByKey(<span class="keyword">False</span>)</span><br><span class="line">rdd = rdd.map(<span class="keyword">lambda</span> v: (v[<span class="number">1</span>],v[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<h2 id="keys-与values"><a href="#keys-与values" class="headerlink" title="keys()与values()"></a>keys()与values()</h2><p><code>keys(),values()</code>：分别把键值对RDD的key和value返回形成一个新的RDD。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获得上一步排序结果的key或value</span></span><br><span class="line">rdd = rdd.keys()</span><br><span class="line">rdd = rdd.values()</span><br></pre></td></tr></table></figure></p>
<h2 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues()"></a>mapValues()</h2><p><code>mapValues(func)</code>：可以对键值对RDD每个元素的value加载到预定义函数进行操作，而不改变key。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 例如对词频统计结果值都减去1</span></span><br><span class="line">rdd = rdd.mapValues(<span class="keyword">lambda</span> v: v - <span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="join"><a href="#join" class="headerlink" title="join()"></a>join()</h2><p><code>join(otherRDD,[numPartitions])</code>：与关系数据库查询一样，表示内连接，给定两个键值对RDD如(K,V1)和(K,V2)，对于两个数据集都存在的key才对其输出，得到一个新的RDD，元素为(K,(V1,V2))。除此以外，还包括其他情形：</p>
<ul>
<li>fullOuterJoin(otherRDD,[numPartitions]) 全连接</li>
<li>leftOuterJoin(otherRDD,[numPartitions]) 左外连接</li>
<li>rightOuterJoin(otherRDD,[numPartitions]) 右外连接</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">'spark'</span>,<span class="number">1</span>),(<span class="string">'hadoop'</span>,<span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">'spark'</span>,<span class="number">1</span>)])</span><br><span class="line">rdd1 = rdd1.join(rdd2)</span><br><span class="line"><span class="comment"># 最后结果为 [('spark', (1,1))]</span></span><br></pre></td></tr></table></figure>
<h1 id="RDD的行动操作（Action）"><a href="#RDD的行动操作（Action）" class="headerlink" title="RDD的行动操作（Action）"></a>RDD的行动操作（Action）</h1><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions" target="_blank" rel="noopener">官方API文档</a>详细列出行动操作函数，下面简单介绍常用的行动操作：</p>
<h2 id="count"><a href="#count" class="headerlink" title="count()"></a>count()</h2><p><code>count()</code>：返回RDD数据集中元素的个数。</p>
<h2 id="collect"><a href="#collect" class="headerlink" title="collect()"></a>collect()</h2><p><code>collect()</code>：以数组的形式返回RDD数据集的所有元素。</p>
<h2 id="first"><a href="#first" class="headerlink" title="first()"></a>first()</h2><p><code>first()</code>：返回RDD数据集的第一个元素。</p>
<h2 id="top"><a href="#top" class="headerlink" title="top()"></a>top()</h2><p><code>top(num,key=None)</code>：以数组的形式返回RDD数据集的前num个元素，默认按<strong>降序</strong>，或者通过key函数指定。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">5</span>,<span class="number">24</span>,<span class="number">3</span>,<span class="number">12</span>,<span class="number">46</span>])</span><br><span class="line">print(rdd.takeOrdered(<span class="number">3</span>))</span><br><span class="line"><span class="comment"># 输出 [46,24,12]</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">5</span>,<span class="number">24</span>,<span class="number">3</span>,<span class="number">12</span>,<span class="number">46</span>])</span><br><span class="line">print(rdd.takeOrdered(<span class="number">3</span>, key=str))</span><br><span class="line"><span class="comment"># 输出 [5,46,3]</span></span><br></pre></td></tr></table></figure></p>
<h2 id="take"><a href="#take" class="headerlink" title="take()"></a>take()</h2><p><code>take(num)</code>：以数组的形式返回RDD数据集的前num个元素。</p>
<h2 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered()"></a>takeOrdered()</h2><p><code>takeOrdered(num,key=None)</code>：以数组的形式返回RDD数据集的前num个元素，默认按<strong>升序</strong>排序，或者通过key函数指定。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">print(rdd.takeOrdered(<span class="number">3</span>))</span><br><span class="line"><span class="comment"># 输出 [1,2,3]</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">print(rdd.takeOrdered(<span class="number">3</span>, key=<span class="keyword">lambda</span> x: -x))</span><br><span class="line"><span class="comment"># 输出 [5,4,3]</span></span><br></pre></td></tr></table></figure></p>
<h2 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample()"></a>takeSample()</h2><p><code>takeSample(withReplacement,num,seed=None)</code>：对RDD数据集进行采样，并以数组的形式返回，withReplacement参数表示是否放回抽样；num参数表示抽样个数；seed表示随机种子。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">print(rdd.takeSample(<span class="keyword">False</span>,<span class="number">3</span>,seed=<span class="number">2019</span>))</span><br></pre></td></tr></table></figure></p>
<h2 id="lookup"><a href="#lookup" class="headerlink" title="lookup()"></a>lookup()</h2><p><code>lookup(key)</code>：以数组的形式返回键值对RDD中键为key的所有值，如果RDD数据集经过特定转换操作按照key进行了分区，那么此行动操作效率会很高。</p>
<h2 id="foreach"><a href="#foreach" class="headerlink" title="foreach()"></a>foreach()</h2><p><code>foreach(func)</code>：将RDD数据集中的每个元素加载到指定函数进行操作，无返回值。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对转换结果逐个输出</span></span><br><span class="line">rdd.foreach(<span class="keyword">print</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="reduce"><a href="#reduce" class="headerlink" title="reduce()"></a>reduce()</h2><p><code>reduce(func)</code>：通过指定函数（如求和、统计）对RDD数据集元素进行聚合。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对所有元素进行求和</span></span><br><span class="line">sum = rdd.reduce(<span class="keyword">lambda</span> v1,v2: v1 + v2)</span><br></pre></td></tr></table></figure></p>
<h2 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate()"></a>aggregate()</h2><p><code>aggregate(zeroValue,seqOp,combOp)</code>：对RDD数据集的元素进行聚合，不要求返回值类型与RDD类型一致；</p>
<ul>
<li>zeroValue: U 给定初始值，形式与最终返回值U一致；</li>
<li>seqOp: (U,V) =&gt; U 对分区内的元素进行聚合（操作发生在每个分区内部）；</li>
<li>combOp: (U,U) =&gt; U 对不同分区的聚合结果做进一步的聚合（操作发生在全部分区的聚合结果间）；</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求一个数组元素的均值</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">res = rdd.aggregate((<span class="number">0</span>,<span class="number">0</span>), <span class="keyword">lambda</span> u,v: (u[<span class="number">0</span>]+v,u[<span class="number">1</span>]+<span class="number">1</span>), <span class="keyword">lambda</span> u1,u2: (u1[<span class="number">0</span>]+u2[<span class="number">0</span>],u1[<span class="number">1</span>]+u2[<span class="number">1</span>]))</span><br><span class="line">print(res[<span class="number">0</span>]/res[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h2 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey()"></a>countByKey()</h2><p><code>countByKey()</code>：以字典的形式返回键值对RDD数据集中每个键的元素的统计数，即(K,count)<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">"rdd"</span>,<span class="number">1</span>),(<span class="string">"rdd"</span>,<span class="number">2</span>),(<span class="string">"spark"</span>,<span class="number">2</span>)])</span><br><span class="line">res = rdd.countByKey()</span><br><span class="line"><span class="comment"># 输出结果 &#123;'rdd': 2, 'spark': 1&#125;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="countByValue"><a href="#countByValue" class="headerlink" title="countByValue()"></a>countByValue()</h2><p><code>countByValue()</code>：以字典的形式返回RDD数据集中每个元素的统计数，即(V,count)<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">res = rdd.countByValue()</span><br><span class="line"><span class="comment"># 输出结果 &#123;2: 2, 3: 1, 1: 2&#125;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile()"></a>saveAsTextFile()</h2><p><code>saveAsTextFile(path, compressionCodecClass=None)</code>：把RDD数据集保存为文本文件，并可以指定是否压缩。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f = NamedTemporaryFile(delete=<span class="keyword">True</span>)</span><br><span class="line">f.close()</span><br><span class="line">codec = <span class="string">"org.apache.hadoop.io.compress.GzipCodec"</span></span><br><span class="line">sc.parallelize([<span class="string">'spark'</span>, <span class="string">'rdd'</span>]).saveAsTextFile(f.name, codec)</span><br></pre></td></tr></table></figure></p>
<h1 id="RDD的持久化"><a href="#RDD的持久化" class="headerlink" title="RDD的持久化"></a>RDD的持久化</h1><ul>
<li>由于RDD采用惰性机制，每次遇到行动操作都会根据DAG的依赖关系从头开始执行计算，如果遇到迭代计算，需要重复调用中间数据，会造成极大的计算开销；</li>
<li>可以通过持久化操作来解决以上的问题，用<code>persist()</code>方法对需要重复使用的RDD标记为持久化，当遇到第一次行动操作后，会把计算结果持久化，保存在计算节点的内存备用；</li>
</ul>
<h2 id="persist"><a href="#persist" class="headerlink" title="persist()"></a>persist()</h2><p><code>persist(storageLevel)</code>：storageLevel参数表示持久化级别，通过使用不同的级别可以把数据缓存到不同的位置，详见 <a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence" target="_blank" rel="noopener">RDD Persistence</a>；其中使用<code>cache()</code>函数会调用默认的持久化方法，即<code>persist(MEMORY_ONLY)</code>将RDD作为反序列化的对象存储在JVM中。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="string">'spark'</span>,<span class="string">'rdd'</span>,<span class="string">'hadoop'</span>])</span><br><span class="line">rdd.cache()</span><br><span class="line">print(rdd.take(<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 第一次行动操作，触发完整计算，同时把rdd放入缓存</span></span><br><span class="line"><span class="comment"># 输出 ['spark']</span></span><br><span class="line">print(rdd.collect())</span><br><span class="line"><span class="comment"># 第二次行动操作，此时直接重复利用上述的缓存rdd</span></span><br><span class="line"><span class="comment"># 输出 ['spark','rdd','hadoop']</span></span><br></pre></td></tr></table></figure></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">yxnchen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yxnchen.github.io/technique/Spark笔记-玩转RDD操作/">http://yxnchen.github.io/technique/Spark笔记-玩转RDD操作/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yxnchen.github.io" target="_blank">YXN's Blog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Big-Data/">Big-Data</a><a class="post-meta__tags" href="/tags/Spark/">Spark</a><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/RDD/">RDD</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/technique/Windows平台下单机Spark环境搭建/"><span>Windows平台下单机Spark环境搭建</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By yxnchen</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Welcome to my blog!</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>